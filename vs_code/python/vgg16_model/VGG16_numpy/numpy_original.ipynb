{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "input_image = torch.randn(batch_size, 3, 32, 32)\n",
    "\n",
    "conv_weight11 = torch.randn(64, 3, 3, 3)\n",
    "conv_bias11 = torch.randn(64)\n",
    "conv_weight12 = torch.randn(64, 64, 3, 3) # (num kernel, depth, i, j)\n",
    "conv_bias12 = torch.randn(64)\n",
    "\n",
    "conv_weight21 = torch.randn(128, 64, 3, 3)\n",
    "conv_bias21 = torch.randn(128)\n",
    "conv_weight22 = torch.randn(128, 128, 3, 3)\n",
    "conv_bias22 = torch.randn(128)\n",
    "\n",
    "conv_weight31 = torch.randn(256, 128, 3, 3)\n",
    "conv_bias31 = torch.randn(256)\n",
    "conv_weight32 = torch.randn(256, 256, 3, 3)\n",
    "conv_bias32 = torch.randn(256)\n",
    "conv_weight33 = torch.randn(256, 256, 3, 3)\n",
    "conv_bias33 = torch.randn(256)\n",
    "\n",
    "conv_weight41 = torch.randn(512, 256, 3, 3)\n",
    "conv_bias41 = torch.randn(512)\n",
    "conv_weight42 = torch.randn(512, 512, 3, 3)\n",
    "conv_bias42 = torch.randn(512)\n",
    "conv_weight43 = torch.randn(512, 512, 3, 3)\n",
    "conv_bias43 = torch.randn(512)\n",
    "\n",
    "conv_weight51 = torch.randn(512, 512, 3, 3)\n",
    "conv_bias51 = torch.randn(512)\n",
    "conv_weight52 = torch.randn(512, 512, 3, 3)\n",
    "conv_bias52 = torch.randn(512)\n",
    "conv_weight53 = torch.randn(512, 512, 3, 3)\n",
    "conv_bias53 = torch.randn(512)\n",
    "\n",
    "fc_weight1 = torch.randn(4096, 512*7*7)\n",
    "fc_bias1 = torch.randn(4096)\n",
    "fc_weight2 = torch.randn(4096, 4096)\n",
    "fc_bias2 = torch.randn(4096)\n",
    "fc_weight3 = torch.randn(10, 4096)\n",
    "fc_bias3 = torch.randn(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "1\n",
      "torch.Size([256, 256, 3, 3])\n",
      "64\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(input_image.shape)\n",
    "print(input_image.shape[0])\n",
    "print(conv_weight32.shape)\n",
    "print(len(conv_weight11))\n",
    "print(conv_bias11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero padding logic before each convolution\n",
    "def zero_padding(input_image):\n",
    "    batch_size = input_image.shape[0]\n",
    "    in_channel = input_image.shape[1]\n",
    "    num_row = input_image.shape[2]\n",
    "    num_col = input_image.shape[3]\n",
    "\n",
    "    padding_image = torch.zeros(batch_size, in_channel, num_row + 2, num_col + 2)\n",
    "\n",
    "    for bc in range(batch_size): # take each in_feature\n",
    "        for ch in range(in_channel): # take each in channel\n",
    "            for row in range(num_row):\n",
    "                for col in range(num_col):\n",
    "                    padding_image[bc][ch][row + 1][col + 1] = input_image[bc][ch][row][col]\n",
    "                    \n",
    "    return padding_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_numpy(input_image, conv_weight, conv_bias):\n",
    "    input_image = zero_padding(input_image)\n",
    "    \n",
    "    batch_size = input_image.shape[0]\n",
    "    in_channel = input_image.shape[1]\n",
    "    num_row = input_image.shape[2] - 2\n",
    "    num_col = input_image.shape[3] - 2\n",
    "\n",
    "    num_kernel = conv_weight.shape[0]\n",
    "    conv_image = torch.zeros(batch_size, num_kernel, num_row, num_col) \n",
    "    for bc in range(batch_size): # take each in_feature\n",
    "        for nk in range(num_kernel): # take each kernel\n",
    "            for row in range(num_row): # take each row\n",
    "                for col in range(num_col): # take each col\n",
    "                    sum = 0\n",
    "                    for ch in range(in_channel): # take each in channel\n",
    "                        for i in range(3):\n",
    "                            for j in range(3):\n",
    "                                sum += input_image[bc][ch][row + i][col + j] * conv_weight[nk][ch][i][j]\n",
    "                                \n",
    "                    sum += conv_bias[nk]\n",
    "                    conv_image[bc][nk][row][col] = sum\n",
    "                    \n",
    "    return conv_image\n",
    "                            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU logic after each convolution\n",
    "def relu(input_image):\n",
    "    batch_size = input_image.shape[0]\n",
    "    in_channel = input_image.shape[1]\n",
    "    num_row = input_image.shape[2]\n",
    "    num_col = input_image.shape[3]\n",
    "\n",
    "    relu_image = torch.zeros(batch_size, in_channel, num_row, num_col)\n",
    "\n",
    "    for bc in range(batch_size): # take each in_feature\n",
    "        for ch in range(in_channel): # take each in channel\n",
    "            for row in range(num_row):\n",
    "                for col in range(num_col):\n",
    "                    relu_image[bc][ch][row][col] = input_image[bc][ch][row][col] if (input_image[bc][ch][row][col] > 0) else 0\n",
    "                    \n",
    "    return relu_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPooling logic, pool_size=2, stride=2\n",
    "def maxpool(input_image):\n",
    "    batch_size = input_image.shape[0]\n",
    "    in_channel = input_image.shape[1]\n",
    "    num_row = input_image.shape[2]\n",
    "    num_col = input_image.shape[3]\n",
    "\n",
    "    pool_image = torch.zeros(batch_size, in_channel, num_row / 2, num_col / 2)\n",
    "\n",
    "    for bc in range(batch_size): # take each in_feature\n",
    "        for ch in range(in_channel): # take each in channel\n",
    "            for row in range(0, num_row, 2):\n",
    "                for col in range(0, num_col, 2):\n",
    "                    max_value = 0\n",
    "                    for i in range(2):\n",
    "                        for j in range(2):\n",
    "                            if max_value < input_image[bc][ch][row + i][col + j]:\n",
    "                                max_value = input_image[bc][ch][row + i][col + j]\n",
    "                    \n",
    "                    pool_image[bc][ch][row / 2][col / 2] = max_value\n",
    "                    \n",
    "    return pool_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input_image):\n",
    "    batch_size = input_image.shape[0]\n",
    "    in_channel = input_image.shape[1]\n",
    "    num_row = input_image.shape[2]\n",
    "    num_col = input_image.shape[3]\n",
    "\n",
    "    flatten_image = torch.zeros(batch_size, in_channel*num_row*num_col)\n",
    "\n",
    "    for bc in range(batch_size): # take each in_feature\n",
    "        for ch in range(in_channel): # take each in channel\n",
    "            for row in range(num_row):\n",
    "                for col in range(num_col):\n",
    "                    flatten_image[bc][in_channel*num_row*num_col + row*num_col + col] = input_image[bc][ch][row][col]\n",
    "                    \n",
    "    return flatten_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(in_image, fc_weight, fc_bias):\n",
    "    batch_size = in_image.shape[0]\n",
    "    in_size = in_image.shape[1] # length\n",
    "    \n",
    "    out_size = fc_weight.shape[0]\n",
    "    \n",
    "    out_image = torch.zeros(batch_size, out_size)\n",
    "    \n",
    "    for bc in range(batch_size):\n",
    "        for i in range(out_size):\n",
    "            sum = 0\n",
    "            for j in range(in_size):\n",
    "                sum += in_image[bc][j] * fc_weight[i][j]\n",
    "        \n",
    "            out_image[bc][i] = sum + fc_bias[i]\n",
    "            \n",
    "    \n",
    "    return out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntensor = relu(tensor)\\ntensor = conv2d_numpy(tensor, conv_weight12, conv_bias12)\\ntensor = relu(tensor)\\ntensor = maxpool(tensor)\\n\\n# Layer 2\\ntensor = conv2d_numpy(tensor, conv_weight21, conv_bias21)\\ntensor = relu(tensor)\\ntensor = conv2d_numpy(tensor, conv_weight22, conv_bias22)\\ntensor = relu(tensor)\\ntensor = maxpool(tensor)\\n\\n# Layer 3\\ntensor = conv2d_numpy(tensor, conv_weight31, conv_bias31)\\ntensor = relu(tensor)\\ntensor = conv2d_numpy(tensor, conv_weight32, conv_bias32)\\ntensor = relu(tensor)\\ntensor = conv2d_numpy(tensor, conv_weight33, conv_bias33)\\ntensor = relu(tensor)\\ntensor = maxpool(tensor)\\n\\n# Layer 4\\ntensor = conv2d_numpy(tensor, conv_weight41, conv_bias41)\\ntensor = relu(tensor)\\ntensor = conv2d_numpy(tensor, conv_weight42, conv_bias42)\\ntensor = relu(tensor)\\ntensor = conv2d_numpy(tensor, conv_weight43, conv_bias43)\\ntensor = relu(tensor)\\ntensor = maxpool(tensor)\\n\\n# Layer 5\\ntensor = conv2d_numpy(tensor, conv_weight51, conv_bias51)\\ntensor = relu(tensor)\\ntensor = conv2d_numpy(tensor, conv_weight52, conv_bias52)\\ntensor = relu(tensor)\\ntensor = conv2d_numpy(tensor, conv_weight53, conv_bias53)\\ntensor = relu(tensor)\\ntensor = maxpool(tensor)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolution test\n",
    "# input_image : (1, 3, 32, 32)\n",
    "tensor = conv2d_numpy(input_image, conv_weight11, conv_bias11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Convolution test\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "tensor = conv2d_numpy(input_image, conv_weight11, conv_bias11)\n",
    "tensor = relu(tensor)\n",
    "tensor = conv2d_numpy(tensor, conv_weight12, conv_bias12)\n",
    "tensor = relu(tensor)\n",
    "tensor = maxpool(tensor)\n",
    "\n",
    "# Layer 2\n",
    "tensor = conv2d_numpy(tensor, conv_weight21, conv_bias21)\n",
    "tensor = relu(tensor)\n",
    "tensor = conv2d_numpy(tensor, conv_weight22, conv_bias22)\n",
    "tensor = relu(tensor)\n",
    "tensor = maxpool(tensor)\n",
    "\n",
    "# Layer 3\n",
    "tensor = conv2d_numpy(tensor, conv_weight31, conv_bias31)\n",
    "tensor = relu(tensor)\n",
    "tensor = conv2d_numpy(tensor, conv_weight32, conv_bias32)\n",
    "tensor = relu(tensor)\n",
    "tensor = conv2d_numpy(tensor, conv_weight33, conv_bias33)\n",
    "tensor = relu(tensor)\n",
    "tensor = maxpool(tensor)\n",
    "\n",
    "# Layer 4\n",
    "tensor = conv2d_numpy(tensor, conv_weight41, conv_bias41)\n",
    "tensor = relu(tensor)\n",
    "tensor = conv2d_numpy(tensor, conv_weight42, conv_bias42)\n",
    "tensor = relu(tensor)\n",
    "tensor = conv2d_numpy(tensor, conv_weight43, conv_bias43)\n",
    "tensor = relu(tensor)\n",
    "tensor = maxpool(tensor)\n",
    "\n",
    "# Layer 5\n",
    "tensor = conv2d_numpy(tensor, conv_weight51, conv_bias51)\n",
    "tensor = relu(tensor)\n",
    "tensor = conv2d_numpy(tensor, conv_weight52, conv_bias52)\n",
    "tensor = relu(tensor)\n",
    "tensor = conv2d_numpy(tensor, conv_weight53, conv_bias53)\n",
    "tensor = relu(tensor)\n",
    "tensor = maxpool(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# fc connected\n",
    "tensor = flatten(tensor)\n",
    "tensor = fc_layer(tensor, fc_weight1, fc_bias1)\n",
    "tensor = relu(tensor)\n",
    "tensor = fc_layer(tensor, fc_weight2, fc_bias2)\n",
    "tensor = relu(tensor)\n",
    "tensor = fc_layer(tensor, fc_weight3, fc_bias3)\n",
    "\n",
    "print(tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
